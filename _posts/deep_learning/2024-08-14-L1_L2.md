---
layout: post
title: 'L1、L2 范数与 L1、L2 损失函数'
date: 2024-08-14
author: 洪茬铭
cover: 'https://pic.imgdb.cn/item/66bcbb4fd9c307b7e9ffc894.png'
tags: 深度学习
---



# 1. 范数

**[范数](https://zh.wikipedia.org/wiki/%E8%8C%83%E6%95%B0)**（英语：Norm），是具有“长度”概念的函数、泛函分析及相关的数学领域，是一个函数，其为向量空间内的所有向量赋予非零的正**长度**或**大小**。

在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算 $AX=B$，可以将向量 $X$ 变化为 $B$，矩阵范数就是来度量这个变化大小的。[1]

## 1.1 Lp-范数（[明氏距离](https://zh.wikipedia.org/wiki/%E6%98%8E%E6%B0%8F%E8%B7%9D%E7%A6%BB)）

Lp-范数是向量空间中的**一组**范数。定义如下：


$$
L_p(\vec{x})=\|\vec{x}\|_p=\Big(\sum_{i=1}^n|x_i|^p\Big)^{1/p},\quad\vec{x}=\{x_1,x_2,\ldots,x_n\}, p\geqslant1. \tag 1
$$

**$$p$$ 的不同取值：**

- $$p=-\infty$$:  
$$
\|\vec{x}\|_{-\infty}=\lim_{p\to-\infty}\left(\sum_{i=1}^n|x_i|^p\right)^{1/p}=\min_i|x_i \tag{2}
$$

- $$p=0$$:  
$$
\|\vec{x}\|_0=\sum_{i=1}^n\left[x_i\neq0\right] \tag{3}
$$
​      也就是所有 $$x_i$$ 中，不等于零的个数。注意，这里的 L0-范数并非通常意义上的范数（不满足[三角不等式](https://zh.wikipedia.org/wiki/三角不等式)或[次可加性](https://zh.wikipedia.org/wiki/次可加性))

- $$p=1$$:  
$$
\|\vec{x}\|_1=\sum_{i=1}^n|x_i|  \tag{4}
$$
​      即 L1-范数是向量各分量绝对值之和

- $$p=2$$:  
$$
\|\vec{x}\|_{2}=\sqrt{\sum_{i=1}^{n}|x_{i}|^{2}} \tag{5}
$$
​      即欧式距离

- $$p=+\infty$$:  
$$
\|\vec{x}\|_{\infty}=\lim_{p\to+\infty}\left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{1/p}=\max_{i}|x_{i}|  \tag{6}
$$
​      此即无穷范数或最大范数，亦称[切比雪夫距离](https://zh.wikipedia.org/wiki/%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB)

<img src="https://pic.imgdb.cn/item/66bca030d9c307b7e9d7341f.png" style="zoom:50%;" />

<center>图中的 q 代表 Lp 公式中的 p，二维空间上的 Lp-范数等高线的其中一条</center>


## 1.2 L1-范数（[曼哈顿距离](https://zh.wikipedia.org/wiki/%E6%9B%BC%E5%93%88%E9%A0%93%E8%B7%9D%E9%9B%A2)）与 L2-范数（[欧氏距离](https://zh.wikipedia.org/wiki/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB)）

L1-范数与 L2-范数是 Lp-范数分别在 p=1 与 p=2 时的情况。

L1-范数的定义如下:
$$
\|\vec{x}\|_1=\sum_{i=1}^n|x_i| \tag{7}
$$

表示向量 $$x$$ 中非零元素的绝对值之和。

L2-范数的定义如下：

$$
\|\vec{x}\|_{2}=\sqrt{\sum_{i=1}^{n}|x_{i}|^{2}} \tag{8}
$$

表示向量元素的平方和再开平方。 

## 1.3 L1 正则化与 L2 正则化

L1-范数与 L2-范数通常会被用来做目标函数的正则化项，防止模型为了迎合训练集而过于复杂造成过拟合的情况，从而提高模型的泛化能力。

由于 L1-范数的天然性质，对 L1 优化的解是一个**稀疏解**， 因此 L1-范数也被叫做**稀疏规则算子**。 通过 L1 可以实现特征的稀疏，去掉一些没有信息的特征，例如在对用户的电影爱好做分类的时候，用户有 100 个特征，可能只有十几个特征是对分类有用的，大部分特征如身高体重等可能都是无用的，利用 L1-范数就可以过滤掉。**因此，L1 正则化通过向成本函数中添加 L1-范数，使得学习得到的结果满足稀疏化，从而方便提取特征。L2 正则化则通常产生平滑的权值，防止过拟合，提升模型的泛化能力。**



### **1.3.1 L1 和 L2 正则先验分别服从什么分布？** 


L1 正则化通过在目标函数中添加参数的绝对值总和来惩罚模型的复杂性，其形式为：

$$
\text{Loss} = \text{Data Loss} + \lambda \sum_{i} |w_i| \tag{9}
$$

其中，$$\lambda$$ 是正则化强度的超参数，$$w_i$$ 是模型的参数。

拉普拉斯分布（双指数分布）的概率密度函数（PDF）为：

$$
P(w) = \frac{1}{2b} \exp\left(-\frac{|w - \mu|}{b}\right) \tag{10}
$$

其中，$$\mu$$ 是分布的平均值，$$b$$ 是比例参数。在最常见的情况下，我们假设 $$\mu = 0$$，则分布简化为：

$$
P(w) = \frac{1}{2b} \exp\left(-\frac{|w|}{b}\right) \tag{11}
$$

在贝叶斯框架中，正则化项可以视为一个先验分布。贝叶斯定理告诉我们，[后验分布与先验分布和似然函数成正比](https://zh.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86)：

$$
\text{Posterior} \propto \text{Likelihood} \times \text{Prior} \tag{12}
$$

- **似然函数（Likelihood）**：基于数据的损失函数。
- **先验分布（Prior）**：对于参数的初始假设或偏好。

L1 正则化中的 $$\lambda \sum_{i} |w_i$$ 项在贝叶斯框架中等价于对参数 $$w$$ 施加了拉普拉斯先验分布。当我们最大化后验概率时，等价于最小化负对数似然加上负对数先验分布的形式，即：

$$
\text{Loss} = -\log(\text{Likelihood}) - \log(\text{Prior}) \tag{13}
$$

对于拉普拉斯先验分布，我们有：

$$
-\log(P(w)) = -\log\left(\frac{1}{2b} \exp\left(-\frac{|w|}{b}\right)\right) = \frac{|w|}{b} + \text{const} \tag{14}
$$

**<font color='red'>由于拉普拉斯分布的形状在零附近陡峭，而在远离零的区域较平缓，因此在估计过程中，这种先验会倾向于让参数 $$w$$ 尽可能地接近零，从而产生稀疏的参数解。这就是为什么 L1 正则化常被用于特征选择和产生稀疏模型。</font>**

L2 正则化与高斯先验分布的推导同上。**<font color='red'>由于高斯分布的形状在零附近较为平缓且对称，在估计过程中，这种先验会让参数 $$w$$ 分布在零附近，但不会完全趋于零。与 L1 正则化相比，L2 正则化更倾向于将参数分布均匀地收缩到较小的值，而不是将很多参数推向零。这使得 L2 正则化适合处理特征数量很多但对性能影响不大的情况。</font>**下图分别为拉普拉斯分布与高斯分布。

<center><img src="https://pic.imgdb.cn/item/66bca949d9c307b7e9dfd715.png" style="zoom: 25%;" /></center>
<center><img src="https://pic.imgdb.cn/item/66bca8efd9c307b7e9df6c42.png" style="zoom:130%;" /></center>

### 1.3.2 L1 正则为什么更容易获得稀疏解？[3]

假设只有一个参数为 $$w$$，损失函数为 $$L(w)$$，分别加上 L1 正则项和 L2 正则项后有：

$$
\mathrm J_{\mathrm L1}(\mathrm w)=\mathrm L(\mathrm w)+\lambda|\mathrm w| \tag{15}
$$

$$
\mathrm{J_{L2}\left(w\right)=L\left(w\right)+\lambda w^2}  \tag{16}
$$

假设 $$L(w)$$ 在 0 处的倒数为 $$d_0$$，即

$$
\left.\frac{\partial\mathrm{L}(\mathrm{w})}{\partial\mathrm{w}}\right|_{\mathrm{w}=0}=\mathrm{d}_0  \tag{17}
$$

则可以推导使用L1正则和L2正则时的导数。

引入 L2 正则项，在 0 处的导数：
$$
\left.\frac{\partial\mathrm J_{\mathrm L2}\left(\mathrm w\right)}{\partial\mathrm w}\right|_{\mathrm w=0}=\mathrm d_0+2\times\lambda\times\mathrm w=\mathrm d_0  \tag{18}
$$

引入 L1 正则项，在 0 处的导数：

$$
\left.\frac{\partial\mathrm{J}_{\mathrm{L}1}\left(\mathrm{w}\right)}{\partial\mathrm{w}}\right|_{\mathrm{w}=0^-}=\mathrm{d}_0-\lambda  \tag{19}
$$

$$
\left.\frac{\partial\mathrm{J}_{\mathrm{L}1}\left(\mathrm{w}\right)}{\partial\mathrm{w}}\right|_{\mathrm{w}=0^+}=\mathrm{d}_0+\lambda  \tag{20}
$$

可见，引入 L2 正则时，代价函数在 0 处的导数仍是 $$d_0$$，无变化。而引入 L1 正则后，代价函数在 0 处的导数有一个突变。从 $$d_0 + \lambda$$ 到 $$d_0 - \lambda$$，若 $$d_0 + \lambda$$ 和 $$d_0 - \lambda$$ 异号，则在 0 处会是一个**极小值点**。因此，优化时，很可能优化到该极小值点上，即 $w=0$ 处。

**<font color='blue'>两种正则化能不能把最优的 $$w$$ 变成 0，取决于原先的损失函数在 0 点处的导数。如果本来导数不为 0，那么施加 L2 正则后导数依然不为 0，最优的 $$w$$ 也不会变成 0。而施加 L1 正则时，只要正则项的系数 $$\lambda$$ 大于原先损失函数在 0 点处的导数的绝对值，$$w=0$$ 就会变成一个极小值点。</font>**[2]

这里只解释了有一个参数的情况，如果有更多的参数，也是类似的。因此，用 L1 正则更容易产生稀疏解。



# 2. 损失函数

L1 损失（也称为绝对值损失）定义为预测值与真实值之差的绝对值的总和。对于一个包含 $$n$$个样本的数据集，L1 损失可以表示为：
$$
L1 = \sum_{i=1}^{n} |y_i - \hat{y}_i| \tag{21}
$$
L2 损失（也称为均方误差，Mean Squared Error, MSE）定义为预测值与真实值之差的平方的总和，再除以样本数 $n$。对于一个包含 $$n$$ 个样本的数据集，L2 损失可以表示为：
$$
L2 = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \tag{22}
$$

其中：

- $$y_i$$ 是第 $$i$$ 个样本的真实值。
- $$\hat{y}_i$$ 是第 $$i$$ 个样本的预测值。

## 2.1 L1 与 L2 的关键区别

1. **对异常值的敏感度**：
   - L1 损失对异常值（outliers）不敏感，因为它计算的是绝对误差，不会因为大误差平方而扩大影响。
   - L2 损失**对异常值敏感**，因为它将误差平方，这意味着大误差会被放大，从而对模型参数产生更大的影响。
2. **优化特性**：
   - L1 损失的优化问题通常会导致**稀疏解**，这意味着某些参数可能被完全驱动为零（特征选择）。**<font color='blue'>由于 L1 损失函数的梯度绝对值处处相等（不考虑原点处不可导），所以对于不同大小的误差，梯度的幅度都是一致的，这使得每个误差的参数更新步长在同一学习率下是相同的。这种一致的梯度更新特性，使得小的参数更容易被驱动到零，因为在靠近零的地方，梯度依然保持不变。</font>**
   - L2 损失的优化问题会倾向于产生**平滑解**，每个参数都受到一定程度的惩罚而缩小，但不会完全为零。**<font color='blue'>对于L2损失函数（平方损失）而言，靠近零的梯度会越来越小，因此参数不容易被驱动到零。这也是 L1 正则化有助于产生稀疏解而 L2 正则化有助于产生平滑解的原因之一。</font>**
3. **使用场景**：
   - L1 损失常用于稀疏数据场景或特征选择。
   - L2 损失更常用于需要平滑解的场景。

## 2.2 损失函数在图像超分辨率中的应用

对于图像超分辨率领域来说，由于从低分辨率图像扩展到高分辨率图像时，往往产生的像素值是接近的，理论上来说采用 L2 损失函数是比较合适的，可以达到更高的 PSNR 值，但是目前大多数的图像超分辨率的方法使用的都是 L1 损失函数。[Zhao等人](https://www.researchgate.net/profile/Michael-Elad/publication/220942248_On_Single_Image_Scale-Up_Using_Sparse-Representations/links/5a64b36aaca272a1581f160c/On-Single-Image-Scale-Up-Using-Sparse-Representations.pdf)实验发现，与其他损失函数相比，L2 损失训练在 PSNR 和 SSIM 方面并不能保证更好的性能。在他们的实验中，用 L1 训练的网络比用 L2 训练的网络取得了更好的性能。图像超分中的 [EDSR/MDSR](https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf) 方法论文中也指出“我们用 L1 损失而不是 L2 来训练我们的网络。最小化 L2 通常是首选，因为它可以最大化 PSNR 。然而，基于一系列实验，我们经验地发现 L1 损失比 L2 损失提供更好的收敛性。”





# 参考资料

[1] [范数（norm） 几种范数的简单介绍](https://blog.csdn.net/a493823882/article/details/80569888)

[2] [L1正则化比L2正则化易得稀疏解的三种解释](https://sm1les.com/2019/01/07/l1-and-l2-regularization/)

[3] [L1正则为什么更容易获得稀疏解](https://blog.csdn.net/b876144622/article/details/81276818)

